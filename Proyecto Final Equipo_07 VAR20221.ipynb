{"cells":[{"cell_type":"markdown","metadata":{"id":"EE80E8guscPE"},"source":["# Detección de plantas a partir de imágenes"]},{"cell_type":"markdown","metadata":{"id":"10mPhIMf1fgS"},"source":["### Integrantes"]},{"cell_type":"markdown","metadata":{"id":"dhE4j9XQ1iL-"},"source":["\n","\n","*   Javier de Jesús Silva (Ing. Sistemas e informática)\n","*   Santiago Salazar Ramirez (Ing. Sistemas e informática)\n","*   Jonathan Marcillo Pantoja (Ing. Agricola)\n","*   Juan José Garcia Marquez (Ing. Control) \n","*   Alejandro Olivares Restrepo (Ing Mecanica)\n","*   Juan Sebastián Durán Roldán (Ing. Sistemas e informática)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OE3wdlmK2J36"},"source":["### Descripción"]},{"cell_type":"markdown","metadata":{"id":"-Oq9zSd-35wp"},"source":["<p>Actualmente existe una gran relevancia en el campo, dado que es uno de los sectores más relevantes en la economía nacional, como también es uno de los más preocupantes a la hora de hablar de modernización de métodos de producción y de distribución. Con esta problemática en mente se fundó la idea de realizar una identificación de plantas, esto como base para futuros análisis. </p>"]},{"cell_type":"markdown","metadata":{"id":"fhudRsMy5YQy"},"source":["<p>Las diferentes formas, colores y posiciones que pueden tomar las plantas es el principal problema de nuestra investigación, siendo poco uniforme la forma de identificar las diferentes plantas según la imagen en donde se tenga que identificar. Otro problema sería los factores ambientales que rodean a la planta, es decir, ¿en qué condiciones se encuentra el alrededor de la vegetación? Podría afectar la luminosidad que le llega a la planta, la humedad del ambiente, la cantidad de objetos diferentes dispersos cerca a la planta, etc. Con esto en mente formamos la pregunta de esta investigación: ¿qué es lo esencial de una planta para identificar lo que es la planta? </p>"]},{"cell_type":"markdown","metadata":{"id":"RGFIx5cH7fvV"},"source":["<p>Es importante clasificar las plantas, debido a que de esta manera comprendemos su funcionalidad, sus cualidades, características y podemos comprender el comportamiento natural, así como utilizar a favor las propiedades de la misma.\n","\n","Las plantas tienen muchas cualidades importantes, algunas medicinales, otras generadores y otras consumibles. El clarificarlas nos permite entender cómo se comportan las plantas y cómo pueden ser útiles. </p>"]},{"cell_type":"markdown","metadata":{"id":"NR4Yq2UX2M5y"},"source":["<p>A raíz de esto nace la idea de seleccionar y clasificar las plantas de manera automatizada con el uso de visión artificial en el proceso, esto es un componente importante por la gran variedad de utilidades que posee, por ejemplo, la detección de objetos y la evaluación de resultados. </p>"]},{"cell_type":"markdown","metadata":{"id":"0Ho83YgF-m2X"},"source":["### ¿Cómo vamos a resolver esto?"]},{"cell_type":"markdown","metadata":{"id":"sGaOc7qtCfWZ"},"source":["¿Por qué? : Hoy en día algunas empresas utilizan materiales que son nocivos para la salud del trabajador \"fungicidas\", lo cual genera ambientes hostiles que imposibilitan el desempeño ideal. Además del ambiente, el factor humano afecta el proceso de revisión, los cuales pueden ser ocasionados por el mismo estado anímico o cansancio físico del colaborador, surgiendo entonces esta alternativa: el uso de algoritmos de supervisión para seleccionarlas."]},{"cell_type":"markdown","metadata":{"id":"mi74ZvWhHuty"},"source":["¿Cómo? : Para la toma de imágenes de las plantas es necesario un espacio de trabajo  en donde la interferencia sea mínima, garantizando así gran detalle en toda el área de la hoja, y poder hacer una buena clasificación de la misma.  Luego se procede a convertir las imágenes a diferentes espacios de color y analizar la información que se puede obtener de la imagen a partir de cada uno de los canales analizados, para lo cual, se procede a analizar también su respectivo histograma y poder realizar un mejor análisis.\n"]},{"cell_type":"markdown","metadata":{"id":"bHCAuAmJbQJK"},"source":["¿Para qué?: Se implementará un software de reconocimiento con visión artificial para la selección de hojas de plantas, identificando áreas donde se encuentren estas, una para cada una. Con el objetivo de automatizar este proceso de selección ya sea para procesos industriales donde se tenga que identificar zonas con plantaciones o calcular el nivel de vegetación en un area."]},{"cell_type":"markdown","metadata":{"id":"SAoYm5iZSRtN"},"source":["###Objetivos"]},{"cell_type":"markdown","metadata":{"id":"EEpiAYrNSUwT"},"source":["\n","1.   Seleccionar y clasificar plantas de manera automatizada a través del uso de la visión artificial.​\n","\n","2.   Evaluar diferentes tipos de algoritmos de detección y reconocimiento de patrones que permitan detectar plantas.\n","\n","3.   Encontrar aplicativos en varias plantas y entornos a fin de crear un algoritmo que pueda detectar sin importar variables de video o fotografia.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IfBL_RoySfb_"},"source":["### Contribuciones"]},{"cell_type":"markdown","metadata":{"id":"9ByB5AZXSigk"},"source":["Se analizaron los diferentes canales de color,  para posteriormente, aislar la planta del medio donde se encuentra, utilizando los umbrales del filtro verde  y utilizando grafos para limpiar la imagen. Se buscó el pixel verde que esté más arriba, más a la izquierda, más arriba a la derecha y más arriba a la izquierda. Luego de ello se hacen recuadros de cada componente del grafo, para identificar las plantas de la imagen. ​"]},{"cell_type":"markdown","metadata":{"id":"EDjYK9DX8rU3"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"eSqXw3CM9Iq7"},"source":["<p>Lo más importante por anunciar: algunas imágenes usadas en este trabajo NO son de nuestra creación, son suministradas gracias a Minervini, Scharr y Tsaftaris; es gracias a ellos que el siguiente análisis fue posible. La composición de las plantas se basa de una variedad de pequeñas plantas con una diferente cantidad distinta de hojas, donde cada planta es diferente en cuanto a las formas de sus hojas y su cantidad, pero su base no difiere entre ellas. <br> La cantidad de imágenes suministradas por Minervini son más de 500, pero se usaron 5 para la caracterización de las imágenes. El mayor limitante a la hora de realizar todo el proceso es distinguir la planta de su contorno, puesto sus colores son similares entre sí. <br> Además de estas imágenes se añadieron otras tomadas por el mismo equipo de trabajo, junto a unos cuantos vídeos para demostrar la detección de plantas.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZYA2Q0Usa6m"},"outputs":[],"source":["%matplotlib inline\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"PAVy8Y2xwWbA"},"source":["## Análisis canales RGB"]},{"cell_type":"markdown","metadata":{"id":"gOR3_u3YwgCa"},"source":["<p> El proceso consiste en leer las imágenes, para luego por cada imagen mostrar 3 imágenes según el canal correspondiente (RGB). Este proceso se realizó con el objetivo de identificar en qué canal la planta se distingue del ambiente. Cabe aclarar que se aplicó una corección de gama, para poder aplicar en condiciones similares las plantas (algunas plantas tienen diferencia en la luz del ambiente, haciendo necesario este paso). <p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMVzVbI_H9zX"},"outputs":[],"source":["# Funciones para aplicar cambio en luz\n","#Definir la función para aplicar la transformación sobre la imagen RGB\n","def apply_f_on_rgb(img, f, args):\n","    \n","    #Crear una matriz de ceros del tamaño de la imagen de entrada\n","    res = np.zeros(img.shape, np.uint8)\n","    #Aplicar la transformación f sobre cada canal del espacio de color RGB\n","    res[:,:,0] = f(img[:,:,0], *args)\n","    res[:,:,1] = f(img[:,:,1], *args)\n","    res[:,:,2] = f(img[:,:,2], *args)\n","    \n","    return res\n","\n","#Definir la función de transformación de la imagen (corrección gamma)\n","def gamma_correction(img, a, gamma):\n","    \n","    #Crear copia de la imagen tipo flotante dada la normalización\n","    img_copy = img.copy().astype(np.float32)/255.0\n","    #La función corrección gamma es de la forma ax^gamma, donde x es la imagen de entrada\n","    res_gamma = cv2.pow(img_copy,gamma)\n","    res = cv2.multiply(res_gamma, a)\n","    \n","    #Asegurar que la los datos queden entre 0 y 255 y sean uint8\n","    res[res<0] = 0\n","    res = res*255.0\n","    res[res>255] = 255\n","    \n","    res = res.astype(np.uint8)\n","    \n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fQsoRC66FUX9","outputId":"fa017dca-cbd8-469e-d69b-3aa5e974062c"},"outputs":[],"source":["#Canal RGB\n","# Paths de las diferentes imagenes\n","img_path = [\"res/img1.png\", \"res/img2.png\", \"res/img3.png\", \"res/img4.png\",\"res/img6.jpg\",\"res/img5.png\",\"res/img8.jpg\",\"res/img9.jpg\",\"res/img10.jpg\"]\n","\n","# Se hace una lista para ir agregando las diferentes imagenes ya leídas, no solo el path.\n","loaded_img = []\n","# El brillo aumentado, para usar las listas a las que se les va a aplicar la función de gamma\n","brillo_aumentado = []\n","\n","for p in img_path:\n","  loaded_img.append(cv2.imread(p, cv2.IMREAD_COLOR))\n","# Se carga cada imagen y se le aplica la función\n","for img in loaded_img:\n","    #Dar valor a los parámetros a,gamma (args)\n","  a = 1\n","  gamma = 0.75\n","  # cv2.cvtColor(img, cv2.COLOR_BGR2XYZ)\n","  #Aplicar la transformación corrección gamma sobre la imagen de entrada\n","  b = apply_f_on_rgb(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), gamma_correction, [a, gamma])\n","  brillo_aumentado.append(b)\n","\n","# Se hace un for por cada imagen y se crean 3 imagenes en sus tres distintos canales con la función de brillo\n","for img_rgb in brillo_aumentado:\n","    img_R, img_G, img_B = img_rgb[:,:,0], img_rgb[:,:,1], img_rgb[:,:,2]\n","    # El subplot es para agregar las 4 imagenes: 3 de los distintos canales y la imagen original\n","    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(30, 8))\n","    # Se desactiva el axis, dado que no es necesario verlo como si se tratase de un plano cartesiano.\n","    ax1.axis(\"off\")\n","    ax2.axis(\"off\")\n","    ax3.axis(\"off\") \n","    ax4.axis(\"off\") \n","    # Los subtitulos para que se pueda notar cual es cada imagen\n","    fig.suptitle('RGB', fontsize=20)\n","    ax1.set_title('[RGB] Canal R')\n","    ax1.imshow(img_R, cmap='Reds', aspect='auto')\n","    ax2.set_title('[RGB] Canal G')\n","    ax2.imshow(img_G, cmap='Greens', aspect='auto')\n","    ax3.set_title('[RGB] Canal B')\n","    ax3.imshow(img_B, \"Blues\", aspect='auto')\n","    ax4.set_title('Imagen original')\n","    ax4.imshow(img_rgb, aspect='auto')"]},{"cell_type":"markdown","metadata":{"id":"ni-C5huXxeyX"},"source":["## Análisis canales LAB"]},{"cell_type":"markdown","metadata":{"id":"7z6J3wRLxkRd"},"source":["<p> Como ya se tienen las imágenes no es necesario volver a tomar del path, sino que meramente se hace un análisis de los canales posibles con LAB, para luego elegir cual es el mejor canal para la distinción de la planta en la imagen. </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BENCmXMOPL5b","outputId":"d762fe05-62ba-49a1-9983-361c865aa8dd"},"outputs":[],"source":[" #Canal LAB\n","\n","channel_A_imgs = []\n","# Un for en todas las imagenes de brillo aumentado\n","for img in brillo_aumentado:\n","    #img = img[140:140 + 20, 120:120 + 20]\n","    # Se pasa a tipo img_lab\n","    img_lab  = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n","    img_L, img_A, img_B = img_lab[:,:,0], img_lab[:,:,1], img_lab[:,:,2]\n","\n","    # Se agregan a esta lista porque se decidió más tarde que este es el mejor canal para hacer el análisis posterior \n","    channel_A_imgs.append(img_A)\n","    fig, ((ax1, ax2, ax3, ax4), (his1, his2, his3, his4)) = plt.subplots(2,4, figsize=(25, 15))\n","    ax1.axis(\"off\")\n","    ax2.axis(\"off\")\n","    ax3.axis(\"off\")\n","    ax4.axis(\"off\")\n","\n","    #Canales\n","    fig.suptitle('Canal LAB', fontsize=20)\n","    ax1.set_title('[LAB] Canal Luminosidad')\n","    ax1.imshow(img_L, aspect='auto')\n","    ax2.set_title('[LAB] Canal A*')\n","    ax2.imshow(img_A, aspect='auto')\n","    ax3.set_title('[LAB] Canal B*')\n","    ax3.imshow(img_B, aspect='auto')\n","    ax4.set_title('Formato original')\n","    ax4.imshow(img_lab, aspect='auto')\n","\n","    # Histogramas\n","    his1.set_title('[LAB] Histograma canal luminosidad')\n","    hisdata = img_L.ravel()\n","    his1.hist(hisdata,histtype='step', bins=255, range=(0.0, 255.0),density=True)\n","    his2.set_title('[LAB] Histograma canal A*')\n","    hisdata = img_A.ravel()\n","    his2.hist(hisdata,histtype='step', bins=255, range=(0.0, 255.0),density=True)\n","    his3.set_title('[LAB] Histograma canal B*')\n","    hisdata = img_B.ravel()\n","    his3.hist(hisdata,histtype='step', bins=255, range=(0.0, 255.0),density=True)\n","    his4.set_title('Formato original')\n","    hisdata = img_lab.ravel()\n","    his4.hist(hisdata,histtype='step', bins=255, range=(0.0, 255.0),density=True)"]},{"cell_type":"markdown","metadata":{"id":"ORNMUQfGzdcD"},"source":["## Análisis canales XYZ"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ErsoTHTPaaoJ","outputId":"2bb2c76d-90e2-4808-8b24-70c52848cee0"},"outputs":[],"source":["  #Canal XYZ\n","# For en las imagenes\n","for img in brillo_aumentado:\n","    # Pasar a XYZ\n","    img_xyz  = cv2.cvtColor(img, cv2.COLOR_BGR2XYZ)\n","    img_X, img_Y, img_Z = img_xyz[:,:,0], img_xyz[:,:,1], img_xyz[:,:,2]\n","    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(30, 8))\n","    ax1.axis(\"off\")\n","    ax2.axis(\"off\")\n","    ax3.axis(\"off\") \n","    ax4.axis(\"off\") \n","    fig.suptitle('Canal XYZ', fontsize=20)\n","    ax1.set_title('Canal X')\n","    # Se usa gray para una mejor visualización\n","    ax1.imshow(img_X, cmap = \"gray\", aspect='auto')\n","    ax2.set_title('Canal Y')\n","    ax2.imshow(img_Y, cmap = \"gray\", aspect='auto')\n","    ax3.set_title('Canal Z')\n","    ax3.imshow(img_Z, cmap = \"gray\", aspect='auto')\n","    ax4.set_title('Formato original')\n","    ax4.imshow(img_xyz,  aspect='auto')"]},{"cell_type":"markdown","metadata":{"id":"KjRmG4i60hGH"},"source":["## Aislamiento de la planta"]},{"cell_type":"markdown","metadata":{"id":"ztrgwmrulehS"},"source":["### Métricas"]},{"cell_type":"markdown","metadata":{"id":"eLX3tvyllZ8V"},"source":["| Canales                 | Identificación                                                                                                                                                                            | Ventajas                                                                                                                                                                  | Desventajas                                                                                                        | Análisis                                                                                                                                                   |\n","|-------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n","| [RGB] Canal R           |<ul><li>Las plantas no poseen una diferenciación en el canal rojo con respecto a su ambiente.</li><li>A menos que sea una planta de color rojo no existe utilidad real.</li></ul>                                 |         <ul><li>Para nuestro caso particular: No existe ventaja alguna. No existen muchas plantas de color rojo.</li></ul>                                                                | <ul><li>No va a identificar la mayoría de plantas.</li></ul>                                                                       | Es preferible no manejar este canal para la identificación de plantas                                                                                      |\n","| [RGB] Canal G           | <ul><li>Las plantas se diferencian del ambiente.</li><li>La mayoría de plantas son verdes, siendo este color de un tono más oscuro a comparación del ambiente.</li></ul>                                        | <ul><li>Diferenciación entre la planta y el ambiente.</li> <li> Rango de valores manejables para la definición de planta.</li></ul>                                                               | <ul><li> Algunos objetos que no son plantas se identifican como tal.</li></ul>                                                      | Este fue el método seleccionado, pero con una leve modificación para poder identificar qué objetos podrían ser plantas (más adelante se explica el método) |\n","| [RGB] Canal B           | <ul><li>Surge el mismo problema que el canal rojo: no se diferencia la planta del ambiente.</li> <li>¿Qué tan común es encontrar una planta de tono azul? No existe una utilidad real.</li>  </ul>               | <ul><li> No tiene ventaja alguna.</li></ul>                                                                                                                                                | <ul><li> No va a identificar planta alguna.</li> <li>En caso de emplearlo sólo funcionaría para casos específicos.</li></ul>               | Tampoco se va a implementar esta solución.                                                                                                                 |\n","| [LAB] Canal Luminosidad | <ul><li>Distingue la planta del ambiente, pero algunos objetos del ambiente también son identificados.</li> <li>Es dependiente de la iluminación del objeto, hace difícil la distinción entre plantas.</li></ul> |  <ul><li>Distingue la planta del ambiente. </li></ul>                                                                                                                                      | <ul><li>Esa distinción es dependiente del ambiente, no es lineal la selección de la planta.</li></ul>                             | Hace compleja la identificación de la planta, dado que es demasiado dependiente de las variables ambientales.                                              |\n","| [LAB] Canal A*          | <ul><li>Existe una diferencia clara entre la planta y el ambiente.</li> <li>A simple vista parece ser un candidato potencial para la futura identificación de plantas.</li></ul>                               | <ul><li>Distingue la planta del ambiente.</li> <li>La cantidad de objetos identificados que no son plantas es mínimo.</li> <li>El rango de valores para la planta es conciso y no tan amplio.</li></ul> | <ul><li>En algunas ocasiones no extrae la planta deseada según la intensidad de píxel deseada.</li></ul>                      | En su momento fue una opción empleada para la identificación de la planta, ahora con la diferenciación de verdes se prefirió no emplear este.              |\n","| [LAB] Canal B*          | <ul><li>En algunos casos identifica la planta, en otros siquiera la puede</li></ul> diferenciar.                                                                                                          | <ul><li> En algunos casos distingue la planta del ambiente. </li></ul>                                                                                                                     | <ul><li>En otros casos no puede distinguir la planta del ambiente, es muy dependiente de la luz y el color de la planta.</li> | Descartada por ineficiente para identificar plantas verdes.                                                                                                |\n","| [XYZ] Canal X,Y,Z       | <ul><li> Se resumieron en una sola fila por la ineficiencia de estos canales: no puede identificar siquiera la planta.</li></ul>                                                                           |<ul><li> Ninguna. </li></ul>                                                                                                                                                               | <ul><li> Tal vez para otros objetos estos canales pueden ser útiles, pero para este caso específico no tiene función.</li></ul>     | Descartada por su inutilidad.                                                                                                                              |"]},{"cell_type":"markdown","metadata":{"id":"4w677ioo0rMH"},"source":["<p>Este es uno de los puntos más importantes para la correcta detección de las plantas. Para poder tomar la planta como tal tenemos que tomar el canal verde, pero no necesariamente todo color que tenga una gran cantidad de canal verde es verde. ¿Qué significa esto? Que si se toma meramente el canal verde se pueden tomar otros colores que se identifican como verde, como lo sería el amarillo. Una correción para esto es hacer una comparación entre el verde y sus otros dos canales (rojo y ázul), con la comparación adecuada (línea 9) se puede llegar a hacer la primera figura de la planta. </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"d2jLV9pIrzHq","outputId":"0d7c664d-fbb2-492f-d54c-dbaa2e073c3a"},"outputs":[],"source":["# Lista de todas las imagenes con la aislación del verde\n","adjusted_green_channels = []\n","\n","for img_rgb in brillo_aumentado:\n","  rc, gc, bc = img_rgb[:, :, 0], img_rgb[:, :, 1], img_rgb[:, :, 2]\n","  new_gc = gc.copy()\n","\n","  for r in range(gc.shape[0]):\n","    for c in range(gc.shape[1]):\n","      # Aquí se hace una diferencia entre el canal verde y el canal azul y rojo: si este canal verde es mayor a los otros se selecciona (también tiene que ser mayor a 100)\n","      if int(gc[r][c]) > 0.75*int(rc[r][c]) + 0.45*int(bc[r][c]) and int(gc[r][c]) > 100:\n","        new_gc[r][c] = gc[r][c]\n","      else:\n","        # En caso contrario el pixel se vuelve blanco\n","        new_gc[r][c] = 0\n","\n","  adjusted_green_channels.append(new_gc)\n","\n","  fig, ax1 = plt.subplots(1,1, figsize=(10, 8))\n","\n","  fig.suptitle(\"Planta N°\", fontsize = 20)\n","  ax1.imshow(new_gc, \"Greens\", aspect = \"auto\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DBl4w91lYRpD"},"source":["## Implementación de grafos"]},{"cell_type":"markdown","metadata":{"id":"fXrpL9hZYRpE"},"source":["<p>Hay un problema claro en la primera detección de imágenes: existen pixeles alejados de la planta y que siquiera son plantas, lo cual haría que estos pixeles sean identificados como plantas. Los grafos sirven en este ejercicio dado que cada píxel tiene pixeles adyacentes, entonces se considera cada pixel como un nodo y su cercanía inmediata (un cuadrado alrededor del pixel) como las aristas entre dichos nodos. </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35mGCTQIYRpE"},"outputs":[],"source":["import networkx as nx  # Se importa la libreria\n","listaGrafos = [] # Creacion de los grafos de cada imagen\n","\n","for cosa in adjusted_green_channels:\n","    G = nx.Graph()  # Se inicializa el grafo\n","    for r in range(cosa.shape[0]): # r se refiere a row\n","        for c in range(cosa.shape[1]): # c se refiere a column\n","            if cosa[r][c] > 0: # R\n","                posicionNodo = (cosa.shape[1]*r)+c   # Numero_Fila * numeroFilas\n","                G.add_node(posicionNodo, position = [r,c]) # Para resumir se añade la posición a cada nodo, para no tener problema a la hora de usar las coordenadas\n","                if c > 0: # Si la columna es mayor a 0, para ir haciendo las aristas con los elementos atras de él\n","                    if cosa[r][c-1] > 0: # Cada vez que se pone que mayor a 0 significa si no es un pixel blanco, porque la idea es hacer adyacencia con los pixeles de colores\n","                        G.add_edge(posicionNodo,posicionNodo-1)\n","                if r > 0: # Ahora si la fila es mayor a 0, para ir haciendo las aristas con: los elementos detras de él, los elementos detrás de él pero a la izquierda y los elementos detrás de él pero a la derecha\n","                    if cosa[r-1][c] > 0:\n","                        G.add_edge(posicionNodo,posicionNodo-(cosa.shape[1]))\n","                    if c > 0: \n","                        if cosa[r-1][c-1] > 0:\n","                            G.add_edge(posicionNodo,posicionNodo-(cosa.shape[1])-1)\n","                    if c < cosa.shape[1]-1:\n","                        if cosa[r-1][c+1] > 0:\n","                            G.add_edge(posicionNodo,posicionNodo-(cosa.shape[1])+1)\n","    listaGrafos.append(G)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UFiAOJ7HYRpE"},"source":["Ya luego de la creación del grafo se revisan los componentes del grafo, donde si la longitud del componente es inferior a cierta constante (2000) entonces los pixeles de dicho componente se tornan 0 (blancos), para luego borrar estos nodos del grafo. Ya con esto se eliminarían los pixeles alejados que aparecen en la imagen (dato anormal)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQuUpGG_YRpF"},"outputs":[],"source":["for i in range(len(adjusted_green_channels)): # Ahora vamos a quitar los pixeles outliers, es decir, todo pixel que no haga parte de lo que se considere como planta\n","    G = listaGrafos[i] # El grafo \n","    listaEliminar = [] # Además de poner el pixel a 0 se quita del grafo, para no tenerlo ahí\n","    Elemento = nx.connected_components(G) # Los componentes del grafo\n","    for elemento in Elemento: # Hora de iterar sobre los componentes del grafo\n","        if len(elemento) < 2000: # Esta es la condición para eliminar el componente y poner sus pixeles en 0. Si el componente tiene menos de 2000 nodos entonces se quita\n","            lista = list(elemento) # El elemento es un set, toca pasarlo a lista\n","            listaEliminar.append(lista) # Como este se va a eliminar entonces lo metemos a la listaEliminar\n","            listaCoordenadas = [G.nodes[x]['position'] for x in lista] # Vamos a transformar la lista (la lista de nodos) a las coordenadas, que es su atributi\n","            for cosa in listaCoordenadas: # Vamos a iterar sobre la lista de coordenadas para poder poner ese pixel como un pixel blanco, no verde\n","                adjusted_green_channels[i][cosa[0]][cosa[1]] = 0\n","    for elemento in listaEliminar: # Para cada componente que sea menor a 2000 se le quitan sus nodos al grafo\n","        for nodo in elemento:\n","            G.remove_node(nodo)\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjckuMbCYRpF","outputId":"e5da5b7b-b71c-40ba-8bcd-af322b22a320"},"outputs":[],"source":["for clean in adjusted_green_channels:\n","  fig, ax1 = plt.subplots(1,1, figsize=(10, 8))\n","\n","  fig.suptitle(\"Planta N°\", fontsize = 20)\n","  ax1.imshow(clean, \"Greens\", aspect = \"auto\")"]},{"cell_type":"markdown","metadata":{"id":"sPf7v3323Wqi"},"source":["## Detección de las plantas"]},{"cell_type":"markdown","metadata":{"id":"DLry2xX43z9d"},"source":["<p> Como primera instancia se buscan las posiciones Y y X más extremos de cada componente del grafo, considerando que la idea es delimitar con un rectángulo rojo la planta (componente del grafo). Pero existe un problema aquí: se pueden estar considerado plantas a objetos que no lo son, más que todo por una discontinuidad de los componentes del grafo. Como solución se plantea ver la intersección de cada cuadrado de cada componente (con el método de obtener las posiciones extremas), sabiendo que en caso de que exista algún tipo de intersección o que un rectángulo esté contenido dentro de otro se van a juntar estos dos componentes para crear uno solo, así evitando que se formen rectángulos erroneos (se delimiten partes incorrectas). </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOtzCrFXYRpH"},"outputs":[],"source":["# Vamos a cambiar esos sets de los componentes a listas, para luego \n","# poder trabajar con ellos\n","listaComponenteImagen = [] \n","for i in range(len(adjusted_green_channels)):\n","    G = listaGrafos[i]\n","    listaComponente = []\n","    Elemento = nx.connected_components(G)\n","    for elemento in Elemento:\n","        listaComponente.append(list(elemento))\n","    listaComponenteImagen.append(listaComponente)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaG1jdL5YRpI"},"outputs":[],"source":["# La idea de la intersección entre cuadrados es identificar si el punto de un cuadrado está dentro de otro cuadrado. Así es que se dice que existe una intersección o contención del cuadrado\n","def interseccionCuadrados(cuadrado1,cuadrado2):\n","    # cuadrado: [ (),(),(),()]\n","    for ve in cuadrado1:\n","        if cuadrado2[3][0]>ve[0]>cuadrado2[0][0] and cuadrado2[0][1]<ve[1]<cuadrado2[1][1]: # ¿El cuadrado 1 se intersecta o está dentro del cuadrado 2?\n","            return True\n","    for ve in cuadrado2:\n","        if cuadrado1[3][0]>ve[0]>cuadrado1[0][0] and cuadrado1[0][1]<ve[1]<cuadrado1[1][1]: # ¿El cuadrado 2 se intersecta o está dentro del cuadrado 1?\n","            return True\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3ZwcMARYRpI"},"outputs":[],"source":["# Encontrar los cuadrados para cada grafo de cada imagen\n","for i in range(len(listaComponenteImagen)): # Iterar sobre cada imagen\n","    G = listaGrafos[i] # El grafo en cuestión\n","    listaCoordenadasComponente = [] # La lista de coordenadas de cada componente del grafo\n","    for elemento in listaComponenteImagen[i]:\n","        listaCoordenadasComponente.append([G.nodes[x]['position'] for x in elemento]) # Se agregan las coordenadas\n","    listaPuntos = []\n","    for componente in listaCoordenadasComponente:\n","            # Se hallan las posiciones de: Arriba, Abajo, Izquierda y derecha del cuadrado. \n","            posArriba = min(componente,key=lambda x: x[0])[0]\n","            posAbajo = max(componente,key=lambda x: x[0])[0]\n","            posIzquierda = min(componente,key=lambda x: x[1])[1]\n","            posDerecha = max(componente,key=lambda x: x[1])[1]\n","            # Los vertices del cuadrado\n","            vtl = (posArriba,posIzquierda)\n","            vtr = (posArriba,posDerecha)\n","            vbl = (posAbajo,posIzquierda)\n","            vbr = (posAbajo,posDerecha)\n","            listaPuntos.append([vtl,vtr,vbl,vbr])\n","    for j in range(len(listaComponenteImagen[i])-1):\n","        for k in range(j+1,len(listaComponenteImagen[i])):\n","            # Se revisan entre los componentes si existe una intersección entre cuadrados\n","            intersectados = interseccionCuadrados(listaPuntos[j],listaPuntos[k])\n","            if intersectados == True:\n","                # En caso de intersección se une el primer nodo del componente1 y el primer nodo del componente2 -> Ahora sólo hay 1 un componente, se unen los cuadrados\n","                G.add_edge(listaComponenteImagen[i][j][0],listaComponenteImagen[i][k][0])\n","            \n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxIzYZHxYRpI"},"outputs":[],"source":["import matplotlib.patches as patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLPZpcuaYRpI","outputId":"63cf44c9-889e-4ed1-d82c-52cbbd9a25a5"},"outputs":[],"source":["# La idea es crear cuadrados rojos alrededor de cada planta\n","for i in range(len(adjusted_green_channels)): \n","    fig, ax1 = plt.subplots(1,1, figsize=(10, 8))\n","    G = listaGrafos[i] #El grafo de la imagen en cuestion\n","    Elemento = nx.connected_components(G)\n","    for elemento in Elemento:\n","        lista = list(elemento)\n","        listaCoordenadas = [G.nodes[x]['position'] for x in lista] # Se obtienen las coordenadas de cada componente\n","        posArriba = min(listaCoordenadas,key=lambda x: x[0])[0] # Se obtienen las posiciones más arriba, más abajo\n","        posAbajo = max(listaCoordenadas,key=lambda x: x[0])[0]\n","        posIzquierda = min(listaCoordenadas,key=lambda x: x[1])[1] # Más izquierda, más derecha\n","        posDerecha = max(listaCoordenadas,key=lambda x: x[1])[1]\n","        # Esto para crear un cuadrado alrededor del componente (planta)\n","        rect = patches.Rectangle((posIzquierda,posAbajo),posDerecha-posIzquierda,posArriba-posAbajo, linewidth=1, edgecolor='r', facecolor='none')\n","        ax1.add_patch(rect) # Se añade al plot\n","        #listaCuadrado.append([posArriba,posAbajo,posIzquierda,posDerecha])\n","    # Se muestra la imagen\n","    ax1.imshow(cv2.cvtColor(loaded_img[i], cv2.COLOR_BGR2RGB), aspect = \"auto\")"]},{"cell_type":"markdown","metadata":{"id":"nS6NuhUaYRpJ"},"source":["## Detección de plantas en vídeo"]},{"cell_type":"markdown","metadata":{"id":"fuHKPW6QYRpJ"},"source":["<p> Ahora vamos a observar qué tan bien sirve este algoritmo en un vídeo </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLt7ZDXkYRpJ"},"outputs":[],"source":["# Lo que hace es de vídeo a imágenes\n","video = 'videos/tres.mp4'\n","vidcap = cv2.VideoCapture(video)\n","success,image = vidcap.read()\n","count = 0\n","while success:\n","    string = \"videos/3/frame{contar}.jpg\".format(contar = count)\n","    cv2.imwrite(string, image)     # save frame as JPEG file      \n","    success,image = vidcap.read()\n","    count += 1\n","cantidadPlantaVideo = count\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2tFFdFcTYRpJ"},"source":["Se ajusta el brillo para las imagenes del vídeo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZO8wntXYRpK"},"outputs":[],"source":["imagenes_cargadas = []\n","brillo_aumentado = []\n","for i in range(cantidadPlantaVideo):\n","    imagenes_cargadas.append(cv2.imread(\"videos/3/frame{contar}.jpg\".format(contar = i), cv2.IMREAD_COLOR))\n","for img in imagenes_cargadas:\n","    a = 1\n","    gamma = 0.75\n","    # cv2.cvtColor(img, cv2.COLOR_BGR2XYZ)\n","    #Aplicar la transformación corrección gamma sobre la imagen de entrada\n","    b = apply_f_on_rgb(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), gamma_correction, [a, gamma])\n","    brillo_aumentado.append(b)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zf0EFzKqYRpK"},"source":["<p>Se hace el análisis de qué pixeles verdes</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWmkfTXoYRpK"},"outputs":[],"source":["verdeAjustado = []\n","for img_rgb in brillo_aumentado:\n","    rc, gc, bc = img_rgb[:, :, 0], img_rgb[:, :, 1], img_rgb[:, :, 2]\n","    new_gc = gc.copy()\n","    for r in range(gc.shape[0]):\n","        for c in range(gc.shape[1]):\n","            if int(gc[r][c]) > 0.75*int(rc[r][c]) + 0.45*int(bc[r][c]) and int(gc[r][c]) > 100:\n","                new_gc[r][c] = gc[r][c]\n","            else:\n","                new_gc[r][c] = 0\n","    verdeAjustado.append(new_gc)\n"]},{"cell_type":"markdown","metadata":{"id":"J1ZRHa2nYRpK"},"source":["Asignación de los grafos y limpieza en los grafos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_MGEoB7YRpL"},"outputs":[],"source":["import networkx as nx  # Se importa la libreria\n","grafos = [] # Creacion de los grafos de cada imagen\n","\n","for cosa in verdeAjustado:\n","    G = nx.Graph()  # Se inicializa el grafo\n","    for r in range(cosa.shape[0]): # r se refiere a row\n","        for c in range(cosa.shape[1]): # c se refiere a column\n","            if cosa[r][c] > 0: # R\n","                posicionNodo = (cosa.shape[1]*r)+c   # Numero_Fila * numeroFilas\n","                G.add_node(posicionNodo, position = [r,c]) # Para resumir se añade la posición a cada nodo, para no tener problema a la hora de usar las coordenadas\n","                if c > 0: # Si la columna es mayor a 0, para ir haciendo las aristas con los elementos atras de él\n","                    if cosa[r][c-1] > 0: # Cada vez que se pone que mayor a 0 significa si no es un pixel blanco, porque la idea es hacer adyacencia con los pixeles de colores\n","                        G.add_edge(posicionNodo,posicionNodo-1)\n","                if r > 0: # Ahora si la fila es mayor a 0, para ir haciendo las aristas con: los elementos detras de él, los elementos detrás de él pero a la izquierda y los elementos detrás de él pero a la derecha\n","                    if cosa[r-1][c] > 0:\n","                        G.add_edge(posicionNodo,posicionNodo-(cosa.shape[1]))\n","                    if c > 0:\n","                        if cosa[r-1][c-1] > 0:\n","                            G.add_edge(posicionNodo,posicionNodo-(cosa.shape[1])-1)\n","                    if c < cosa.shape[1]-1:\n","                        if cosa[r-1][c+1] > 0:\n","                            G.add_edge(posicionNodo,posicionNodo-(cosa.shape[1])+1)\n","    grafos.append(G)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dEKnG8CYRpL"},"outputs":[],"source":["for i in range(len(verdeAjustado)): # Ahora vamos a quitar los pixeles outliers, es decir, todo pixel que no haga parte de lo que se considere como planta\n","    G = grafos[i] # El grafo \n","    listaEliminar = [] # Además de poner el pixel a 0 se quita del grafo, para no tenerlo ahí\n","    Elemento = nx.connected_components(G) # Los componentes del grafo\n","    for elemento in Elemento: # Hora de iterar sobre los componentes del grafo\n","        if len(elemento) < 1000: # Esta es la condición para eliminar el componente y poner sus pixeles en 0. Si el componente tiene menos de 2000 nodos entonces se quita\n","            lista = list(elemento) # El elemento es un set, toca pasarlo a lista\n","            listaEliminar.append(lista) # Como este se va a eliminar entonces lo metemos a la listaEliminar\n","            listaCoordenadas = [G.nodes[x]['position'] for x in lista] # Vamos a transformar la lista (la lista de nodos) a las coordenadas, que es su atributi\n","            for cosa in listaCoordenadas: # Vamos a iterar sobre la lista de coordenadas para poder poner ese pixel como un pixel blanco, no verde\n","                verdeAjustado[i][cosa[0]][cosa[1]] = 0\n","    for elemento in listaEliminar: # Para cada componente que sea menor a 2000 se le quitan sus nodos al grafo\n","        for nodo in elemento:\n","            G.remove_node(nodo)"]},{"cell_type":"markdown","metadata":{"id":"dhLo4IinYRpL"},"source":["<p>Detección de plantas</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-_lWWlZYRpL"},"outputs":[],"source":["# Vamos a cambiar esos sets de los componentes a listas, para luego \n","# poder trabajar con ellos\n","listacomponente = [] \n","for i in range(len(verdeAjustado)):\n","    G = grafos[i]\n","    listaComponente = []\n","    Elemento = nx.connected_components(G)\n","    for elemento in Elemento:\n","        listaComponente.append(list(elemento))\n","    listacomponente.append(listaComponente)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJnRW369YRpM"},"outputs":[],"source":["# Encontrar los cuadrados para cada grafo de cada imagen\n","for i in range(len(listacomponente)): # Iterar sobre cada imagen\n","    G = grafos[i] # El grafo en cuestión\n","    listaCoordenadasComponente = [] # La lista de coordenadas de cada componente del grafo\n","    for elemento in listacomponente[i]:\n","        listaCoordenadasComponente.append([G.nodes[x]['position'] for x in elemento]) # Se agregan las coordenadas\n","    listaPuntos = []\n","    for componente in listaCoordenadasComponente:\n","            posArriba = min(componente,key=lambda x: x[0])[0]\n","            posAbajo = max(componente,key=lambda x: x[0])[0]\n","            posIzquierda = min(componente,key=lambda x: x[1])[1]\n","            posDerecha = max(componente,key=lambda x: x[1])[1]\n","            vtl = (posArriba,posIzquierda)\n","            vtr = (posArriba,posDerecha)\n","            vbl = (posAbajo,posIzquierda)\n","            vbr = (posAbajo,posDerecha)\n","            listaPuntos.append([vtl,vtr,vbl,vbr])\n","    for j in range(len(listacomponente[i])-1):\n","        for k in range(j+1,len(listacomponente[i])):\n","            intersectados = interseccionCuadrados(listaPuntos[j],listaPuntos[k])\n","            if intersectados == True:\n","                G.add_edge(listacomponente[i][j][0],listacomponente[i][k][0])\n","            \n","    \n"]},{"cell_type":"markdown","metadata":{"id":"x5PhXOTSYRpM"},"source":["<p>Agregar las imagenes </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kg6Q8MGDYRpM"},"outputs":[],"source":["for i in range(len(verdeAjustado)): \n","    fig, ax1 = plt.subplots(1,1, figsize=(10, 8))\n","    G = grafos[i] #El grafo de la imagen en cuestion\n","    Elemento = nx.connected_components(G)\n","    for elemento in Elemento:\n","        lista = list(elemento)\n","        listaCoordenadas = [G.nodes[x]['position'] for x in lista] # Se obtienen las coordenadas de cada componente\n","        posArriba = min(listaCoordenadas,key=lambda x: x[0])[0] # Se obtienen las posiciones más arriba, más abajo\n","        posAbajo = max(listaCoordenadas,key=lambda x: x[0])[0]\n","        posIzquierda = min(listaCoordenadas,key=lambda x: x[1])[1] # Más izquierda, más derecha\n","        posDerecha = max(listaCoordenadas,key=lambda x: x[1])[1]\n","        # Esto para crear un cuadrado alrededor del componente (planta)\n","        rect = patches.Rectangle((posIzquierda,posAbajo),posDerecha-posIzquierda,posArriba-posAbajo, linewidth=1, edgecolor='r', facecolor='none')\n","        ax1.add_patch(rect) # Se añade al plot\n","    ax1.axis('off')\n","    ax1.imshow(cv2.cvtColor(imagenes_cargadas[i], cv2.COLOR_BGR2RGB), aspect = \"auto\")\n","    plt.close(fig)\n","    fig.savefig('videos/imagenes/{numero}.png'.format(numero = i))"]},{"cell_type":"markdown","metadata":{"id":"uAnw2huKYRpM"},"source":["Creación del vídeo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfAdpBTGYRpN"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","\n","img_array = []\n","for i in range(cantidadPlantaVideo):\n","    filename = \"videos/imagenes/{numero}.png\".format(numero = i)\n","    img = cv2.imread(filename)\n","    height, width, layers = img.shape\n","    size = (width,height)\n","    img_array.append(img)\n","\n","\n","out = cv2.VideoWriter('resultado/project1.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n"," \n","for i in range(len(img_array)):\n","    out.write(img_array[i])\n","out.release()"]},{"cell_type":"markdown","metadata":{"id":"Lf-nJlACYYGn"},"source":["## Empleando YOLO"]},{"cell_type":"markdown","metadata":{"id":"pWC5uqXuYck7"},"source":["Esta sección es para probar el cómo funcionaría la identificación de plantas con YOLO con un modelo predeterminado, es un adicionado que se deseó agregar para comparar con nuestra implementación de grafos."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22299,"status":"ok","timestamp":1656454594315,"user":{"displayName":"Santiago Salazar Ramirez","userId":"13975008640868790269"},"user_tz":300},"id":"BfpdtYVIYcRk","outputId":"d2b9ea26-214d-41dd-c830-cab2df31e507"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"Y0yY-M-DY8lX"},"source":["Primer paso: Descarga de Yolo junto a las librerias necesarias"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2637,"status":"ok","timestamp":1656454611603,"user":{"displayName":"Santiago Salazar Ramirez","userId":"13975008640868790269"},"user_tz":300},"id":"LaZwENeJYygP","outputId":"aa77ac60-4573-4641-8d0f-f9f2b90f1891"},"outputs":[],"source":["!git clone https://github.com/ultralytics/yolov5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5475,"status":"ok","timestamp":1656454627604,"user":{"displayName":"Santiago Salazar Ramirez","userId":"13975008640868790269"},"user_tz":300},"id":"ll0mj4NbY6T9","outputId":"c031798a-0251-478d-b002-1e61471c49fd"},"outputs":[],"source":["\n","!pip install -r yolov5/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1656454931505,"user":{"displayName":"Santiago Salazar Ramirez","userId":"13975008640868790269"},"user_tz":300},"id":"YnvmAAyraFkF","outputId":"a2e45a43-a1ed-480a-dfc9-85f62ada3027"},"outputs":[],"source":["%cd yolov5"]},{"cell_type":"markdown","metadata":{"id":"P7ZDBc4VbS3C"},"source":["Guardamos un archivo mp4 para luego emplear YOLO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}}},"executionInfo":{"elapsed":29275,"status":"ok","timestamp":1656455422023,"user":{"displayName":"Santiago Salazar Ramirez","userId":"13975008640868790269"},"user_tz":300},"id":"JZgZCz68aG5A","outputId":"0eec0fa2-a427-4803-f6c8-2726964ab098"},"outputs":[],"source":["from google.colab import files\n","uploaded=files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85171,"status":"ok","timestamp":1656455552959,"user":{"displayName":"Santiago Salazar Ramirez","userId":"13975008640868790269"},"user_tz":300},"id":"KYRblXwYbYL0","outputId":"95212ea1-49ab-4946-ec92-f004e3cda7d4"},"outputs":[],"source":["!python detect.py --source Cinco.mp4"]},{"cell_type":"markdown","metadata":{"id":"3TebBAhVqtfu"},"source":["### Análisis entre YoloV e identificación de verdes"]},{"cell_type":"markdown","metadata":{"id":"PeoMzVSfqyuL"},"source":["| Métodos                   | Análisis                                                                                                                                                                                                                                                                                               | Ventajas                                                                                                                                                                                                                                                                      | Desventajas                                                                                                                                  |\n","|---------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|\n","| Identificación de verdes  | <ul><li>Identifica todo objeto que parezca al verde de una planta.</li> <li>En algunas ocasiones identifica toda la imagen como una planta.</li> <li>Dependiente de la cantidad de luz.</li><li>Aunque no requiera de un gran proceso de entrenamiento sigue teniendo el problema de identificar las plantas de forma eficiente.</li></ul> | <ul><li>No requiere de entrenamiento.</li> <li>Su tiempo de carga es inferior al de entrenamiento de una red neuronal.</li> <li>En algunas ocasiones identifica bien las plantas, esto si cumple con los requisitos de luz y de color.</li></ul>                                                            | <ul><li>Tiempos de carga para los vídeos.</li> <li>Identificación de objetos que no son plantas.</li> <li>Identificación del escenario como una planta completa.</li></ul> |\n","| Yolov5 modelo por defecto | <ul><li>Identifica las plantas que vengan en maceta.</li> <li>Confunde la maceta por una zanahoria u otro tipo de objeto extraño.</li> <li>En algunos casos identifica la planta con otro objeto, como un brócoli.</li> <li>Es dependiente del modelo.</li> <li>Requiere un dataset de entrenamiento grande para su mejora.</li></ul>              | <ul><li>Ya luego de ser entrenada los tiempos de carga son demasiado bajos.</li> <li>Identifica bien algunas plantas que están en maceta, en otros casos salta y hace caso omiso.</li> <li>Tiene la posibilidad de mejora si se realiza un gran dataset de plantas para el entrenamiento (10.000)</li></ul> | <ul><li>Identificación de objetos que no son plantas.</li> <li>Mal uso de labels.</li> <li>El entrenamiento dura es demasiado largo. </li></ul>                            |"]},{"cell_type":"markdown","metadata":{"id":"MTexwsXy5mjv"},"source":["# Conclusiones y resultados\n","Como conclusiones finales logramos identificar las plantas en base a colores relacionados, cumpliendo así nuestro objetivo y facilitando el reconocimientos de estas, independientemente si son videos o imágenes, esto gracias a todas las funciones anteriormente explicada donde cada uno nos va permitiendo identificar y pulir más la lectura del elemento en especifico. durante el proceso de descomposición de los canales del data set, y sus filtros, la elaboración de grafos para poder identificar mejor las formas y la aceptación de esta logramos que esto se desarrollara de manera eficaz.\n","\n","Se realizó la descomposición de las imágenes del dataset en canales RGB, XYZ y LAB donde se observó que en las descomposiciones XYZ y LAB no se observaron grandes diferencias que permitieran segmentar las imágenes, también se concluyó que por las condiciones cromáticas del objeto de estudio , lo más apropiado para segmentar las imágenes es el canal RGB , en especial el canal G(aquel que contiene el color verde característico de las plantas). Con este método se pudo aislar el verde de las plantas, pero no solamente de estas sino el verde de todos los objetos encontrados en la imagen, por lo que se llevó a cabo un filtro de aceptación de cuanto verde era apto para ser identificado como hoja de la planta. Seguido a esto no fue la única dificultad se presentaron píxeles aislados que eran indeseables, a esto se le aplicó un grafo que permitía identificar las conexiones entre píxeles y eliminar los que estén sueltos; posteriormente se solucionó el problema y la imagen de se obtuvo más limpia y con la forma de la planta deseada. Finalmente, ya identificada la forma de la planta, el software procede a encerrar en un cuadro de color rojo los límites.\n"]},{"cell_type":"markdown","metadata":{"id":"k-u-KbkevpsB"},"source":["**REFERENCIAS**"]},{"cell_type":"markdown","metadata":{"id":"D-73xxAevy1M"},"source":["MRUNALINI Badnakhe.et al. An Application of K-Means Clustering and Artificial Intelligence in Pattern Recognition for Crop Diseases. [En línea]. ipcsit. [Consultado el 18 de Marzo de 2017]. Disponible en internet: http://www.ipcsit.com/vol20/26-ICAIT2011-A4023.pdf\n","\n"," KULKARNI Anand.et al. Applying image processing technique to detect plant diseases. [En línea]. researchgate\t[Consultado\tel\t18\tde\tMarzo\tde\t2017].\n"]},{"cell_type":"markdown","metadata":{"id":"0PAiGRZayaj_"},"source":["VIJAYASHREE T. et al. Authentication Of Leaf Image Using Image Processing Technique. [En línea]. arpnjournals [Consultado el 18 de Marzo de 2017]. Disponible en internet: http://www.arpnjournals.com/jeas/research_papers/rp_2015/jeas_0515_2091.pdf"]},{"cell_type":"markdown","metadata":{"id":"PPUqIfwTyiuH"},"source":["UNIVERSIDAD POLITECNICA DE VALENCIA. Desarrollo De Un Sistema De Visión Artificial Para El Control Eficiente De Pulverizadores De Cera En El Tratamineo Pos-Cosecha De La Fruta. [En línea]. horticom. [Consultado el 3 de Marzo de 2017]. Disponible en internet: http://www.horticom.com/pd/imagenes/76/144/76144.pdf10"]},{"cell_type":"markdown","metadata":{"id":"AotNI89cdA2B"},"source":["3CIENCIAS. Visión Artificial Aplicada Al Control De La Calidad. [En línea] 3ciencias.com [Consultado el 18 de Marzo de 2017]. Disponible en internet: https://www.3ciencias.com/wp- content/uploads/2014/12/VISI%C3%93N-ARTIFICIAL-APLICADA-AL-CONTROL-DE-LA- CALIDAD.pdf"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Plantas_Entrega_Final.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"1f89eb7ee7f691a835530315e9cc5d675e01c25354f3226d466fc4ec3dae4d41"}}},"nbformat":4,"nbformat_minor":0}
